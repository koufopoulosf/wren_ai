# Wren AI Setup with Claude (Anthropic)

## What Was Fixed

### The Problem
The application was returning **404 errors** for all natural language queries because:
- Docker Compose was using `ghcr.io/canner/wren-engine-ibis:latest` (a query execution engine)
- This image does **NOT** provide the `/v1/asks` endpoint needed for text-to-SQL conversion
- The actual **Wren AI Service** with AI capabilities was missing

### The Solution
Replaced the incorrect Docker image with the proper Wren AI stack:
- âœ… **wren-ai-service** (v0.29.0) - AI-powered text-to-SQL using Claude
- âœ… **Qdrant** - Vector database for semantic search
- âœ… **Claude (Anthropic)** - LLM for natural language understanding (ONLY external API)
- âœ… **Ollama** - Local embeddings for semantic search (100% local, no API needed)

---

## Architecture

```
User Question
    â†“
Streamlit App (port 8501)
    â†“
Wren AI Service (port 5555)
    â”œâ”€â†’ Claude API (text-to-SQL via LiteLLM) â˜ï¸ ONLY external API
    â”œâ”€â†’ Ollama (local embeddings) ðŸ  100% local
    â”œâ”€â†’ Qdrant (vector database) ðŸ  100% local
    â””â”€â†’ Wren Engine (SQL execution) ðŸ  100% local
         â†“
    PostgreSQL Database ðŸ  100% local
         â†“
    Results back to user
```

---

## Prerequisites

You need **only ONE API key**:

**Anthropic API Key** (required)
- Get it from: https://console.anthropic.com/
- Used for: Natural language to SQL conversion
- Model: `claude-sonnet-4-20250514`

**That's it!** Everything else runs 100% locally:
- âœ… Embeddings: Ollama with `nomic-embed-text` (local, no API)
- âœ… Vector DB: Qdrant (local)
- âœ… SQL Engine: Wren Engine (local)
- âœ… Database: PostgreSQL (local)

> **First Run:** Ollama will download the `nomic-embed-text` model (~270MB) automatically on first start. This takes ~1-2 minutes depending on your internet speed.

---

## Setup Instructions

### 1. Configure Environment Variables

```bash
# Copy the example file
cp .env.example .env

# Edit .env and add your API keys
nano .env  # or use your preferred editor
```

Add your key:
```bash
ANTHROPIC_API_KEY=sk-ant-your-actual-key-here
```

### 2. Start the Services

```bash
# Stop existing services (if running)
docker-compose down

# Pull latest images
docker-compose pull

# Start all services
docker-compose up -d

# Watch the logs to see when ready
docker-compose logs -f wren-ai
```

Wait for these messages:

**First, Ollama starts and downloads the embedding model:**
```
wren-ollama | pulling manifest
wren-ollama | pulling f02dd72bb242... 100%
wren-ollama | pulling c71d239df917... 100%
wren-ollama | verifying sha256 digest
wren-ollama | writing manifest
wren-ollama | success
```

**Then, Wren AI starts:**
```
wren-ai-service | INFO: Application startup complete
wren-ai-service | INFO: Uvicorn running on http://0.0.0.0:5555
```

This usually takes **2-3 minutes** on first start (downloading the embedding model ~270MB).

### 3. Verify Services are Running

```bash
# Check service health
docker-compose ps

# All services should show "healthy" or "Up"
# - postgres: healthy
# - wren-engine: Up
# - qdrant: Up
# - ollama: healthy
# - wren-ai: healthy
# - streamlit-app: Up
```

### 4. Test the Application

1. Open: http://localhost:8501
2. You should see your database schema loaded
3. Try asking a question:
   - "Show top 10 customers by orders"
   - "What was total revenue last month?"
   - "What's our average order value?"

If everything is configured correctly, you should get SQL generated by Claude and results from your database!

---

## Troubleshooting

### Issue: Wren AI service fails to start

**Check logs:**
```bash
docker-compose logs wren-ai
```

**Common causes:**
1. Missing API keys in `.env`
2. Invalid API keys
3. Qdrant not ready (it starts first)

**Solution:**
```bash
# Restart just the AI service
docker-compose restart wren-ai

# Or recreate everything
docker-compose down -v
docker-compose up -d
```

### Issue: Still getting 404 errors

**Verify the endpoint:**
```bash
# Test health endpoint
curl http://localhost:5555/health

# Should return: {"status":"ok"} or similar
```

**Check logs:**
```bash
docker-compose logs streamlit-app | grep WREN_URL
# Should show: WREN_URL=http://wren-ai:5555
```

### Issue: "Query timeout" or slow responses

**Adjust timeouts in config:**
Edit `wren-ai-config.yaml`:
```yaml
llm:
  timeout: 180  # Increase from 120

engine:
  config:
    timeout: 90  # Increase from 60
```

Then restart:
```bash
docker-compose restart wren-ai
```

### Issue: Ollama not downloading embedding model

**Check Ollama logs:**
```bash
docker-compose logs ollama
```

**Manually pull the model:**
```bash
docker exec -it wren-ollama ollama pull nomic-embed-text
```

**Verify model is available:**
```bash
docker exec -it wren-ollama ollama list
# Should show: nomic-embed-text
```

---

## Advanced Configuration

### Using a Different Embedding Model

We're already using Ollama with `nomic-embed-text`, but you can use other models:

Edit `wren-ai-config.yaml`:

```yaml
embedder:
  provider: ollama_embedder
  models:
    - model: all-minilm  # Faster, smaller
      # or model: mxbai-embed-large  # Higher quality
      kwargs:
        base_url: http://ollama:11434
```

Then pull the new model:
```bash
docker exec -it wren-ollama ollama pull all-minilm
docker-compose restart wren-ai
```

### Using a Different Claude Model

Edit `wren-ai-config.yaml`:

```yaml
llm:
  provider: litellm_llm
  models:
    - model: claude-opus-4-20250514  # More powerful
      kwargs:
        temperature: 0
        max_tokens: 4096
```

Available models:
- `claude-sonnet-4-20250514` (recommended - balanced)
- `claude-opus-4-20250514` (most powerful)
- `claude-3-5-sonnet-20240620` (legacy)

### Changing Qdrant Settings

To persist Qdrant data across restarts, the volume is already configured:
```yaml
volumes:
  qdrant_data:/qdrant/storage
```

To reset the vector database:
```bash
docker-compose down
docker volume rm wren_ai_qdrant_data
docker-compose up -d
```

---

## Files Changed

| File | Change |
|------|--------|
| `docker-compose.yml` | Added Qdrant, Ollama; replaced wren-ai image; updated to port 5555 |
| `wren-ai-config.yaml` | **NEW** - Wren AI configuration for Claude + Ollama embeddings |
| `.env.example` | Updated to require ONLY Anthropic API key (removed OpenAI) |

---

## Cost Estimation

**Anthropic (Claude) - Your ONLY Cost:**
- Text-to-SQL queries: ~1,000-3,000 tokens each
- Cost: ~$0.003 per query (using Sonnet 4)
- 100 queries/day = ~$0.30/day = $9/month

**Everything Else - FREE:**
- âœ… Embeddings: Ollama (local, $0)
- âœ… Vector DB: Qdrant (local, $0)
- âœ… SQL Engine: Wren Engine (local, $0)

**Total: ~$9/month** for moderate use (100 queries/day)

> **Infrastructure Cost:** Ollama + Qdrant add ~500MB RAM usage. No GPU required.

---

## Next Steps

1. âœ… Set up your API keys in `.env`
2. âœ… Start the services with `docker-compose up -d`
3. âœ… Test natural language queries
4. âœ… Monitor logs for any issues
5. ðŸ“Š Start asking questions about your data!

---

## Support

If you encounter issues:

1. Check logs: `docker-compose logs <service-name>`
2. Verify health: `docker-compose ps`
3. Test endpoints directly: `curl http://localhost:5555/health`
4. Review this guide's troubleshooting section

For Wren AI specific issues:
- GitHub: https://github.com/Canner/WrenAI
- Docs: https://docs.getwren.ai/

---

**Last Updated:** 2025-11-15
